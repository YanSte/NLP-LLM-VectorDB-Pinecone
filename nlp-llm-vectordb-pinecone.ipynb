{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# | NLP | LLM | VectorDB | Pinecone |\n\n## Natural Language Processing (NLP) and Large Language Models (LLM) with Vector Database Pinecone\n\n![Learning](https://t3.ftcdn.net/jpg/06/14/01/52/360_F_614015247_EWZHvC6AAOsaIOepakhyJvMqUu5tpLfY.jpg)\n\n\n# <b>1 <span style='color:#78D118'>|</span> Overview</b>\n\nIn this section, we are going to try out another vector database, called Pinecone. It has a free tier which you need to sign up for to gain access below.\n\nPinecone is a cloud-based vector database that offers fast and scalable similarity search for high-dimensional data, with a focus on simplicity and ease of use. \n\n![Learning](https://d7umqicpi7263.cloudfront.net/img/product/738798c3-eeca-494a-a2a9-161bee9450b2/310429fb-2ce8-4186-adea-cc619511ac3c.png)\n\n## Library pre-requisites\n\n- pinecone-client\n    - pip install below\n- Spark connector jar file\n    - **IMPORTANT!!** Since we will be interacting with Spark by writing a Spark dataframe out to Pinecone, we need a Spark Connector.\n    - You need to attach a Spark-Pinecone connector `s3://pinecone-jars/0.2.1/spark-pinecone-uberjar.jar` in the cluster you are using. Refer to this [documentation](https://docs.pinecone.io/docs/databricks#setting-up-a-spark-cluster) if you need more information. \n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Setup\n","metadata":{}},{"cell_type":"code","source":"pip install pinecone-client==2.2.2","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:32:28.274853Z","iopub.execute_input":"2023-12-28T21:32:28.275793Z","iopub.status.idle":"2023-12-28T21:32:41.608140Z","shell.execute_reply.started":"2023-12-28T21:32:28.275752Z","shell.execute_reply":"2023-12-28T21:32:41.606535Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pinecone-client==2.2.2\n  Obtaining dependency information for pinecone-client==2.2.2 from https://files.pythonhosted.org/packages/98/17/3675b83dca0a032d2750bf04fbfdf78a6e46fa3056eefc2574cdd14661d9/pinecone_client-2.2.2-py3-none-any.whl.metadata\n  Downloading pinecone_client-2.2.2-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.2) (2.31.0)\nRequirement already satisfied: pyyaml>=5.4 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.2) (6.0.1)\nCollecting loguru>=0.5.0 (from pinecone-client==2.2.2)\n  Obtaining dependency information for loguru>=0.5.0 from https://files.pythonhosted.org/packages/03/0a/4f6fed21aa246c6b49b561ca55facacc2a44b87d65b8b92362a8e99ba202/loguru-0.7.2-py3-none-any.whl.metadata\n  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.2) (4.5.0)\nCollecting dnspython>=2.0.0 (from pinecone-client==2.2.2)\n  Obtaining dependency information for dnspython>=2.0.0 from https://files.pythonhosted.org/packages/f6/b4/0a9bee52c50f226a3cbfb54263d02bb421c7f2adc136520729c2c689c1e5/dnspython-2.4.2-py3-none-any.whl.metadata\n  Downloading dnspython-2.4.2-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.2) (2.8.2)\nRequirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.2) (1.26.15)\nRequirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.2) (4.66.1)\nRequirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.2) (1.24.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone-client==2.2.2) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pinecone-client==2.2.2) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pinecone-client==2.2.2) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pinecone-client==2.2.2) (2023.11.17)\nDownloading pinecone_client-2.2.2-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.1/179.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading loguru-0.7.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: loguru, dnspython, pinecone-client\nSuccessfully installed dnspython-2.4.2 loguru-0.7.2 pinecone-client-2.2.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install sparkmagic\n!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:32:41.610573Z","iopub.execute_input":"2023-12-28T21:32:41.610976Z","iopub.status.idle":"2023-12-28T21:36:31.133462Z","shell.execute_reply.started":"2023-12-28T21:32:41.610948Z","shell.execute_reply":"2023-12-28T21:36:31.132676Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting sparkmagic\n  Downloading sparkmagic-0.21.0.tar.gz (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting hdijupyterutils>=0.6 (from sparkmagic)\n  Downloading hdijupyterutils-0.21.0.tar.gz (5.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting autovizwidget>=0.6 (from sparkmagic)\n  Downloading autovizwidget-0.21.0.tar.gz (9.0 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: ipython>=4.0.2 in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (8.14.0)\nCollecting pandas<2.0.0,>=0.17.1 (from sparkmagic)\n  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (2.31.0)\nRequirement already satisfied: ipykernel>=4.2.2 in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (6.25.1)\nRequirement already satisfied: ipywidgets>5.0.0 in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (7.7.1)\nRequirement already satisfied: notebook>=4.2 in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (6.5.5)\nRequirement already satisfied: tornado>=4 in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (6.3.3)\nCollecting requests_kerberos>=0.8.0 (from sparkmagic)\n  Downloading requests_kerberos-0.14.0-py2.py3-none-any.whl (11 kB)\nRequirement already satisfied: nest_asyncio>1.5.5 in /opt/conda/lib/python3.10/site-packages (from sparkmagic) (1.5.6)\nRequirement already satisfied: plotly>=3 in /opt/conda/lib/python3.10/site-packages (from autovizwidget>=0.6->sparkmagic) (5.16.1)\nCollecting jupyter>=1 (from hdijupyterutils>=0.6->sparkmagic)\n  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (0.1.4)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (1.6.7.post1)\nRequirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (7.4.9)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (5.3.1)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (0.1.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (5.9.3)\nRequirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (24.0.1)\nRequirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.2.2->sparkmagic) (5.9.0)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (0.2.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (0.19.0)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (3.0.39)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (2.16.1)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (0.6.2)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.2->sparkmagic) (4.8.0)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>5.0.0->sparkmagic) (0.2.0)\nRequirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>5.0.0->sparkmagic) (3.6.6)\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>5.0.0->sparkmagic) (3.0.8)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (3.1.2)\nRequirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (21.3.0)\nRequirement already satisfied: nbformat in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (5.9.2)\nRequirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (6.4.5)\nRequirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (1.8.2)\nRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (0.17.1)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (0.17.1)\nRequirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.2->sparkmagic) (1.0.0)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.0.0,>=0.17.1->sparkmagic) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.0.0,>=0.17.1->sparkmagic) (2023.3)\nRequirement already satisfied: cryptography>=1.3 in /opt/conda/lib/python3.10/site-packages (from requests_kerberos>=0.8.0->sparkmagic) (41.0.3)\nCollecting pyspnego[kerberos] (from requests_kerberos>=0.8.0->sparkmagic)\n  Obtaining dependency information for pyspnego[kerberos] from https://files.pythonhosted.org/packages/cc/fd/06a7618de50ad13b7e85115bd1e42c1625e3365313a4c971898386781f89/pyspnego-0.10.2-py3-none-any.whl.metadata\n  Downloading pyspnego-0.10.2-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sparkmagic) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sparkmagic) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->sparkmagic) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sparkmagic) (2023.11.17)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=1.3->requests_kerberos>=0.8.0->sparkmagic) (1.15.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=4.0.2->sparkmagic) (0.8.3)\nRequirement already satisfied: qtconsole in /opt/conda/lib/python3.10/site-packages (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (5.5.1)\nRequirement already satisfied: jupyter-console in /opt/conda/lib/python3.10/site-packages (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (6.6.3)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.2.2->sparkmagic) (0.4)\nRequirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.2.2->sparkmagic) (4.1.0)\nRequirement already satisfied: jupyter-server>=1.8 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (2.12.1)\nRequirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (0.2.3)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (0.8.4)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (0.2.2)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (6.0.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (1.5.0)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (0.6.0)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (4.12.2)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (0.5.13)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (2.1.3)\nRequirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.2->sparkmagic) (2.18.0)\nRequirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.2->sparkmagic) (4.19.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=4.0.2->sparkmagic) (0.7.0)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly>=3->autovizwidget>=0.6->sparkmagic) (8.2.3)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=4.0.2->sparkmagic) (0.2.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas<2.0.0,>=0.17.1->sparkmagic) (1.16.0)\nRequirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook>=4.2->sparkmagic) (21.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ipykernel>=4.2.2->sparkmagic) (3.0.9)\nCollecting gssapi>=1.6.0 (from pyspnego[kerberos]->requests_kerberos>=0.8.0->sparkmagic)\n  Downloading gssapi-1.8.3.tar.gz (94 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting krb5>=0.3.0 (from pyspnego[kerberos]->requests_kerberos>=0.8.0->sparkmagic)\n  Downloading krb5-0.5.1.tar.gz (221 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.1/221.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.2->sparkmagic) (1.2.0)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.2->sparkmagic) (2.2.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.2->sparkmagic) (0.2.2)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=1.3->requests_kerberos>=0.8.0->sparkmagic) (2.21)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (23.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (0.9.2)\nRequirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (3.7.1)\nRequirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (0.9.0)\nRequirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (0.4.4)\nRequirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (6.5.0)\nRequirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (1.6.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.2->sparkmagic) (2.3.2.post1)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert>=5->notebook>=4.2->sparkmagic) (0.5.1)\nRequirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from qtconsole->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (2.4.1)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (1.1.3)\nRequirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (2.0.7)\nRequirement already satisfied: pyyaml>=5.3 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (6.0.1)\nRequirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (0.1.1)\nRequirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (1.5.1)\nRequirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (2.0)\nRequirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (1.3.0)\nRequirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (1.13)\nRequirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (1.2.3)\nDownloading pyspnego-0.10.2-py3-none-any.whl (129 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: sparkmagic, autovizwidget, hdijupyterutils, gssapi, krb5\n  Building wheel for sparkmagic (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sparkmagic: filename=sparkmagic-0.21.0-py3-none-any.whl size=67571 sha256=ef09f411f4eab6b3fe373c73822ad46af90a9c62e0bd3f7be53da865c379b66b\n  Stored in directory: /root/.cache/pip/wheels/cf/e6/43/f1918a8a9679b065cdf7580107213e21346ea7b3e929230a73\n  Building wheel for autovizwidget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for autovizwidget: filename=autovizwidget-0.21.0-py3-none-any.whl size=14670 sha256=370741addb4368d6f016c3906bae16af9b49b273fcedf84306c53324598a1bc1\n  Stored in directory: /root/.cache/pip/wheels/89/a4/8b/8031ab10b516319e98fd9a555a49bd8ae3d3de4c8584ba4626\n  Building wheel for hdijupyterutils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for hdijupyterutils: filename=hdijupyterutils-0.21.0-py3-none-any.whl size=7636 sha256=ef5420741b5ae8fd8cc5171299905666ddb129b0dca79f53c3472035bca73fd4\n  Stored in directory: /root/.cache/pip/wheels/6d/c5/34/5ac56467aa04154f6c148ddd19b9e7771951b2d56009c3108c\n  Building wheel for gssapi (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gssapi: filename=gssapi-1.8.3-cp310-cp310-linux_x86_64.whl size=1111891 sha256=c4c85ece6ec8514213db8ff8aa854214830f6bb81738cc8ae96df6c126ed5288\n  Stored in directory: /root/.cache/pip/wheels/87/3d/a4/a49f352c16a790928eaa1c84e8a8b505f85651bf450de80761\n  Building wheel for krb5 (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for krb5: filename=krb5-0.5.1-cp310-cp310-linux_x86_64.whl size=1183509 sha256=33595ea779038106cbd3696498e18b94b5505b2ba2bb358bf3c771b7d10c5811\n  Stored in directory: /root/.cache/pip/wheels/ce/b7/7c/76519a566acc1920166954bde7c5bff8dd872e8f545e417860\nSuccessfully built sparkmagic autovizwidget hdijupyterutils gssapi krb5\nInstalling collected packages: krb5, gssapi, pandas, pyspnego, requests_kerberos, jupyter, hdijupyterutils, autovizwidget, sparkmagic\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.0.3\n    Uninstalling pandas-2.0.3:\n      Successfully uninstalled pandas-2.0.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbeatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.12.1 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\nfitter 1.6.0 requires pandas<3.0.0,>=2.0.3, but you have pandas 1.5.3 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed autovizwidget-0.21.0 gssapi-1.8.3 hdijupyterutils-0.21.0 jupyter-1.0.0 krb5-0.5.1 pandas-1.5.3 pyspnego-0.10.2 requests_kerberos-0.14.0 sparkmagic-0.21.0\nCollecting pyspark\n  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=424b137f0b6f40139501b42b645611c4e9ee0dd34d2b587b602645b2f25f5f3f\n  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:36:31.134650Z","iopub.execute_input":"2023-12-28T21:36:31.135001Z","iopub.status.idle":"2023-12-28T21:36:44.626750Z","shell.execute_reply.started":"2023-12-28T21:36:31.134969Z","shell.execute_reply":"2023-12-28T21:36:44.619695Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.36.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.0.0+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.15.1+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.24.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.19.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (10.1.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=a9ec71ab43962fc72898e59b295a096acb50661b099bc61c78dce25ddbd6c732\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"cache_dir = \"./cache\"","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:36:44.633664Z","iopub.execute_input":"2023-12-28T21:36:44.634829Z","iopub.status.idle":"2023-12-28T21:36:44.650036Z","shell.execute_reply.started":"2023-12-28T21:36:44.634735Z","shell.execute_reply":"2023-12-28T21:36:44.647648Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_column', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_seq_items', None)\npd.set_option('display.max_colwidth', 500)\npd.set_option('expand_frame_repr', True)","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:36:44.654086Z","iopub.execute_input":"2023-12-28T21:36:44.654810Z","iopub.status.idle":"2023-12-28T21:36:45.344916Z","shell.execute_reply.started":"2023-12-28T21:36:44.654774Z","shell.execute_reply":"2023-12-28T21:36:45.343975Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# <b>2 <span style='color:#78D118'>|</span> Setting up your Pinecone</b>\n\nStep 1: Go to their [home page](https://www.pinecone.io/) and click `Sign Up Free` on the top right corner. \n<br>\nStep 2: Click on `Sign Up`. It's possible that you may not be able to sign up for a new account, depending on Pinecone's availability. \n\n<img src=\"https://files.training.databricks.com/images/pinecone_register.png\" width=300>\n\nStep 3: Once you are in the console, navigate to `API Keys` and copy the `Environment` and `Value` (this is your API key).\n\n<img src=\"https://files.training.databricks.com/images/pinecone_credentials.png\" width=500>\n","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ[\"PINECONE_API_KEY\"] = <FILL IN>\nos.environ[\"PINECONE_ENV\"] = <FILL IN>\"","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:36:45.346118Z","iopub.execute_input":"2023-12-28T21:36:45.346755Z","iopub.status.idle":"2023-12-28T21:36:45.353009Z","shell.execute_reply.started":"2023-12-28T21:36:45.346724Z","shell.execute_reply":"2023-12-28T21:36:45.351569Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import pinecone\n\npinecone_api_key = os.environ[\"PINECONE_API_KEY\"]\npinecone_env = os.environ[\"PINECONE_ENV\"]\n\npinecone.init(api_key=pinecone_api_key, environment=pinecone_env)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:36:45.354718Z","iopub.execute_input":"2023-12-28T21:36:45.355158Z","iopub.status.idle":"2023-12-28T21:36:45.914566Z","shell.execute_reply.started":"2023-12-28T21:36:45.355122Z","shell.execute_reply":"2023-12-28T21:36:45.912711Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <b>3 <span style='color:#78D118'>|</span> Spark setup</b>","metadata":{}},{"cell_type":"code","source":"import pyspark.sql.functions as F\nfrom pyspark.sql import SparkSession\n\n# Spark in local mode else using S3\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\ndf = (\n    spark\n    .read\n    .option(\"header\", True)\n    .option(\"sep\", \";\")\n    .format(\"csv\")\n    .load(\n        f\"/kaggle/input/topic-labeled-news-dataset/labelled_newscatcher_dataset.csv\".replace(\n            \"/dbfs\", \"dbfs:\"\n        )\n    )\n    .withColumn(\"id\", F.expr(\"uuid()\"))\n)\nprint(\"DataFrame Type:\")\ndisplay(df)\nprint(\"\\n\")\nprint(\"DataFrame Contents:\")\ndisplay(df.show(10))","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:36:45.916918Z","iopub.execute_input":"2023-12-28T21:36:45.918841Z","iopub.status.idle":"2023-12-28T21:37:00.866912Z","shell.execute_reply.started":"2023-12-28T21:36:45.918792Z","shell.execute_reply":"2023-12-28T21:37:00.865853Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/12/28 21:36:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"DataFrame Type:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"DataFrame[topic: string, link: string, domain: string, published_date: string, title: string, lang: string, id: string]"},"metadata":{}},{"name":"stdout","text":"\n\nDataFrame Contents:\n+-------+--------------------+--------------------+-------------------+--------------------+----+--------------------+\n|  topic|                link|              domain|     published_date|               title|lang|                  id|\n+-------+--------------------+--------------------+-------------------+--------------------+----+--------------------+\n|SCIENCE|https://www.eurek...|      eurekalert.org|2020-08-06 13:59:45|A closer look at ...|  en|a7a32814-12e3-442...|\n|SCIENCE|https://www.pulse...|            pulse.ng|2020-08-12 15:14:19|An irresistible s...|  en|ba7f94b5-cbad-476...|\n|SCIENCE|https://www.expre...|       express.co.uk|2020-08-13 21:01:00|Artificial intell...|  en|a33f545d-3703-4a8...|\n|SCIENCE|https://www.ndtv....|            ndtv.com|2020-08-03 22:18:26|Glaciers Could Ha...|  en|4daf3542-1374-4d8...|\n|SCIENCE|https://www.thesu...|           thesun.ie|2020-08-12 19:54:36|Perseid meteor sh...|  en|aab4e21c-5af8-48a...|\n|SCIENCE|https://interesti...|interestingengine...|2020-08-08 11:05:45|NASA Releases In-...|  en|20e4043d-8082-4a7...|\n|SCIENCE|https://www.thequ...|        thequint.com|2020-05-28 09:09:46|SpaceX, NASA Demo...|  en|59294720-c3c7-4ea...|\n|SCIENCE|https://www.thesp...|  thespacereview.com|2020-08-10 22:48:23|Orbital space tou...|  en|3ceac204-7a26-493...|\n|SCIENCE|https://www.busin...| businessinsider.com|2020-08-16 00:28:54|Greenland's melti...|  en|367440f7-0220-4b1...|\n|SCIENCE|https://www.thehi...|thehindubusinessl...|2020-08-14 07:43:25|NASA invites engi...|  en|71dc9393-b48a-475...|\n+-------+--------------------+--------------------+-------------------+--------------------+----+--------------------+\nonly showing top 10 rows\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}}]},{"cell_type":"markdown","source":"# <b>4 <span style='color:#78D118'>|</span> Generate embedding and save</b>","metadata":{}},{"cell_type":"markdown","source":"For Pinecone, we need to generate the embeddings first and save it to a dataframe, before we can write it out to Pinecone for indexing. \n\nThere are two ways of doing it: \n- 1. Using pandas DataFrame, apply the single-node embedding model, and upsert to Pinecone in batches\n- 2. Using Spark Dataframe and use pandas UDFs to help us apply the embedding model on batches of data\n","metadata":{}},{"cell_type":"markdown","source":"## Method 1: Upsert to Pinecone in batches","metadata":{}},{"cell_type":"code","source":"pdf = df.limit(1000).toPandas()\ndisplay(pdf.head(10))","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:37:00.868645Z","iopub.execute_input":"2023-12-28T21:37:00.868967Z","iopub.status.idle":"2023-12-28T21:37:01.466638Z","shell.execute_reply.started":"2023-12-28T21:37:00.868935Z","shell.execute_reply":"2023-12-28T21:37:01.464829Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"     topic  \\\n0  SCIENCE   \n1  SCIENCE   \n2  SCIENCE   \n3  SCIENCE   \n4  SCIENCE   \n5  SCIENCE   \n6  SCIENCE   \n7  SCIENCE   \n8  SCIENCE   \n9  SCIENCE   \n\n                                                                                                                                         link  \\\n0                                                                          https://www.eurekalert.org/pub_releases/2020-08/dbnl-acl080620.php   \n1                                               https://www.pulse.ng/news/world/an-irresistible-scent-makes-locusts-swarm-study-finds/jy784jw   \n2                 https://www.express.co.uk/news/science/1322607/artificial-intelligence-warning-machine-learning-algorithm-social-media-data   \n3                                                     https://www.ndtv.com/world-news/glaciers-could-have-sculpted-mars-valleys-study-2273648   \n4                                                               https://www.thesun.ie/tech/5742187/perseid-meteor-shower-tonight-time-uk-see/   \n5                                                    https://interestingengineering.com/nasa-releases-in-depth-map-of-beirut-explosion-damage   \n6                                       https://www.thequint.com/tech-and-auto/spacex-nasa-demo-2-rocket-launch-set-for-saturday-how-to-watch   \n7                                                                                               https://www.thespacereview.com/article/4003/1   \n8                                                       https://www.businessinsider.com/greenland-melting-ice-sheet-past-tipping-point-2020-8   \n9  https://www.thehindubusinessline.com/news/science/nasa-invites-engineering-students-to-help-harvest-water-on-mars-moon/article32352915.ece   \n\n                       domain       published_date  \\\n0              eurekalert.org  2020-08-06 13:59:45   \n1                    pulse.ng  2020-08-12 15:14:19   \n2               express.co.uk  2020-08-13 21:01:00   \n3                    ndtv.com  2020-08-03 22:18:26   \n4                   thesun.ie  2020-08-12 19:54:36   \n5  interestingengineering.com  2020-08-08 11:05:45   \n6                thequint.com  2020-05-28 09:09:46   \n7          thespacereview.com  2020-08-10 22:48:23   \n8         businessinsider.com  2020-08-16 00:28:54   \n9    thehindubusinessline.com  2020-08-14 07:43:25   \n\n                                                                                                  title  \\\n0                                               A closer look at water-splitting's solar fuel potential   \n1                                                An irresistible scent makes locusts swarm, study finds   \n2                        Artificial intelligence warning: AI will know us better than we know ourselves   \n3                                                      Glaciers Could Have Sculpted Mars Valleys: Study   \n4  Perseid meteor shower 2020: What time and how to see the huge bright FIREBALLS over UK again tonight   \n5                                                 NASA Releases In-Depth Map of Beirut Explosion Damage   \n6                                      SpaceX, NASA Demo-2 Rocket Launch Set for Saturday: How to Watch   \n7                                                         Orbital space tourism set for rebirth in 2021   \n8                                     Greenland's melting ice sheet has 'passed the point of no return'   \n9                                 NASA invites engineering students to help harvest water on Mars, Moon   \n\n  lang                                    id  \n0   en  a7a32814-12e3-4425-a046-15c137be1b40  \n1   en  ba7f94b5-cbad-4761-9550-6ebf66cdac2f  \n2   en  a33f545d-3703-4a8a-bc7d-340f157fd860  \n3   en  4daf3542-1374-4d8c-ba57-051509bb5a2c  \n4   en  aab4e21c-5af8-48a6-8737-762a0d22e7b7  \n5   en  20e4043d-8082-4a73-a03c-88aa3bff8b31  \n6   en  59294720-c3c7-4ea4-95a6-c8099e11823a  \n7   en  3ceac204-7a26-4937-ad30-c2553a6f6321  \n8   en  367440f7-0220-4b14-9fa3-4fdea0e0c5d1  \n9   en  71dc9393-b48a-475a-9438-ccec6fb7b4fa  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic</th>\n      <th>link</th>\n      <th>domain</th>\n      <th>published_date</th>\n      <th>title</th>\n      <th>lang</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SCIENCE</td>\n      <td>https://www.eurekalert.org/pub_releases/2020-08/dbnl-acl080620.php</td>\n      <td>eurekalert.org</td>\n      <td>2020-08-06 13:59:45</td>\n      <td>A closer look at water-splitting's solar fuel potential</td>\n      <td>en</td>\n      <td>a7a32814-12e3-4425-a046-15c137be1b40</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SCIENCE</td>\n      <td>https://www.pulse.ng/news/world/an-irresistible-scent-makes-locusts-swarm-study-finds/jy784jw</td>\n      <td>pulse.ng</td>\n      <td>2020-08-12 15:14:19</td>\n      <td>An irresistible scent makes locusts swarm, study finds</td>\n      <td>en</td>\n      <td>ba7f94b5-cbad-4761-9550-6ebf66cdac2f</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SCIENCE</td>\n      <td>https://www.express.co.uk/news/science/1322607/artificial-intelligence-warning-machine-learning-algorithm-social-media-data</td>\n      <td>express.co.uk</td>\n      <td>2020-08-13 21:01:00</td>\n      <td>Artificial intelligence warning: AI will know us better than we know ourselves</td>\n      <td>en</td>\n      <td>a33f545d-3703-4a8a-bc7d-340f157fd860</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SCIENCE</td>\n      <td>https://www.ndtv.com/world-news/glaciers-could-have-sculpted-mars-valleys-study-2273648</td>\n      <td>ndtv.com</td>\n      <td>2020-08-03 22:18:26</td>\n      <td>Glaciers Could Have Sculpted Mars Valleys: Study</td>\n      <td>en</td>\n      <td>4daf3542-1374-4d8c-ba57-051509bb5a2c</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SCIENCE</td>\n      <td>https://www.thesun.ie/tech/5742187/perseid-meteor-shower-tonight-time-uk-see/</td>\n      <td>thesun.ie</td>\n      <td>2020-08-12 19:54:36</td>\n      <td>Perseid meteor shower 2020: What time and how to see the huge bright FIREBALLS over UK again tonight</td>\n      <td>en</td>\n      <td>aab4e21c-5af8-48a6-8737-762a0d22e7b7</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>SCIENCE</td>\n      <td>https://interestingengineering.com/nasa-releases-in-depth-map-of-beirut-explosion-damage</td>\n      <td>interestingengineering.com</td>\n      <td>2020-08-08 11:05:45</td>\n      <td>NASA Releases In-Depth Map of Beirut Explosion Damage</td>\n      <td>en</td>\n      <td>20e4043d-8082-4a73-a03c-88aa3bff8b31</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>SCIENCE</td>\n      <td>https://www.thequint.com/tech-and-auto/spacex-nasa-demo-2-rocket-launch-set-for-saturday-how-to-watch</td>\n      <td>thequint.com</td>\n      <td>2020-05-28 09:09:46</td>\n      <td>SpaceX, NASA Demo-2 Rocket Launch Set for Saturday: How to Watch</td>\n      <td>en</td>\n      <td>59294720-c3c7-4ea4-95a6-c8099e11823a</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>SCIENCE</td>\n      <td>https://www.thespacereview.com/article/4003/1</td>\n      <td>thespacereview.com</td>\n      <td>2020-08-10 22:48:23</td>\n      <td>Orbital space tourism set for rebirth in 2021</td>\n      <td>en</td>\n      <td>3ceac204-7a26-4937-ad30-c2553a6f6321</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>SCIENCE</td>\n      <td>https://www.businessinsider.com/greenland-melting-ice-sheet-past-tipping-point-2020-8</td>\n      <td>businessinsider.com</td>\n      <td>2020-08-16 00:28:54</td>\n      <td>Greenland's melting ice sheet has 'passed the point of no return'</td>\n      <td>en</td>\n      <td>367440f7-0220-4b14-9fa3-4fdea0e0c5d1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>SCIENCE</td>\n      <td>https://www.thehindubusinessline.com/news/science/nasa-invites-engineering-students-to-help-harvest-water-on-mars-moon/article32352915.ece</td>\n      <td>thehindubusinessline.com</td>\n      <td>2020-08-14 07:43:25</td>\n      <td>NASA invites engineering students to help harvest water on Mars, Moon</td>\n      <td>en</td>\n      <td>71dc9393-b48a-475a-9438-ccec6fb7b4fa</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Note: Pinecone free tier only allows one index. If you have existing indices, you need to delete them before you are able create a new index.\n\nWe specify the similarity measure, embedding vector dimension within the index.\n\nRead documentation on how to [create index here](https://docs.pinecone.io/reference/create_index/).\n","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\n# We will use embeddings from this model to apply to our data\nmodel = SentenceTransformer(\n    \"all-MiniLM-L6-v2\", cache_folder=cache_dir\n)  # Use a pre-cached model\n","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:37:01.470137Z","iopub.execute_input":"2023-12-28T21:37:01.470593Z","iopub.status.idle":"2023-12-28T21:37:15.095917Z","shell.execute_reply.started":"2023-12-28T21:37:01.470565Z","shell.execute_reply":"2023-12-28T21:37:15.093578Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b3ddc388f1641c6be16c2f56f1e2dae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a70674e206704328bfa224afc0f451a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef55b6731c7d4e8eb1a5ec66eefa2d06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f05788cea1fe4a92b768bc35304abf69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f1ffc6295f04004a9d9ad9e01eadb28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b083d976d9a49fa8fbd8c1e92278e52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa359b7d3ada4b42b3fa10b812a76cad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99274393e6cf43e18955045e9b50debc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"986ee76e7f0c4a6b8ccace6f3884e383"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52c176d2b2174fa08e4bde8af4a1ff2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"728d17d27b744e21adeba7551da7489a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ea71e768b00485c901a487cab5c3f1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fc08b00b5174c6f9bec802f6fa04c53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee289348a72847d49c68b05fdc34e9fc"}},"metadata":{}}]},{"cell_type":"markdown","source":"Delete the index if it already exists","metadata":{}},{"cell_type":"code","source":"pinecone_index_name = \"news\"\n\nif pinecone_index_name in pinecone.list_indexes():\n    pinecone.delete_index(pinecone_index_name)","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:37:15.098566Z","iopub.execute_input":"2023-12-28T21:37:15.100304Z","iopub.status.idle":"2023-12-28T21:37:20.716183Z","shell.execute_reply.started":"2023-12-28T21:37:15.100191Z","shell.execute_reply":"2023-12-28T21:37:20.714217Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Create the index.\n\nWe specify the index name (required), embedding vector dimension (required), and a custom similarity metric (cosine is the default) when creating our index.\n","metadata":{}},{"cell_type":"code","source":"# only create index if it doesn't exist\nif pinecone_index_name not in pinecone.list_indexes():\n    pinecone.create_index(\n        name=pinecone_index_name,\n        dimension=model.get_sentence_embedding_dimension(),\n        metric=\"cosine\",\n    )","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:37:20.717872Z","iopub.execute_input":"2023-12-28T21:37:20.718203Z","iopub.status.idle":"2023-12-28T21:37:26.453108Z","shell.execute_reply.started":"2023-12-28T21:37:20.718180Z","shell.execute_reply":"2023-12-28T21:37:26.452236Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Now connect to the index","metadata":{}},{"cell_type":"code","source":"pinecone_index = pinecone.Index(pinecone_index_name)","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:37:26.455483Z","iopub.execute_input":"2023-12-28T21:37:26.456101Z","iopub.status.idle":"2023-12-28T21:37:26.464835Z","shell.execute_reply.started":"2023-12-28T21:37:26.456066Z","shell.execute_reply":"2023-12-28T21:37:26.462456Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"When the index has been created, we can now upsert vectors of data records to the index. `Upsert` means that we are writing the vectors into the index. \n\nRefer to this [documentation page](https://docs.pinecone.io/docs/python-client#indexupsert) to look at example code and vectors.\n","metadata":{"execution":{"iopub.status.busy":"2023-12-28T12:53:52.039407Z","iopub.execute_input":"2023-12-28T12:53:52.039784Z","iopub.status.idle":"2023-12-28T12:53:52.047634Z","shell.execute_reply.started":"2023-12-28T12:53:52.039754Z","shell.execute_reply":"2023-12-28T12:53:52.046524Z"}}},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nbatch_size = 1000\n\nfor i in tqdm(range(0, len(pdf[\"title\"]), batch_size)):\n    try:\n        # find end of batch\n        # Bug kaggle with min and Spark added __builtin__\n        i_end = min(i + batch_size, len(pdf[\"title\"]))\n        # create IDs batch\n        ids = [str(x) for x in range(i, i_end)]\n        # create metadata batch\n        metadata = [{\"title\": title} for title in pdf[\"title\"][i:i_end]]\n        # create embeddings\n        embedding_title_batch = model.encode(pdf[\"title\"][i:i_end]).tolist()\n        # create records list for upsert\n        records = zip(ids, embedding_title_batch, metadata)\n        # upsert to Pinecone\n        pinecone_index.upsert(vectors=records)\n\n    except Exception as e:\n        print(f\"Error processing batch: {e}\")\n\n# check number of records in the index\npinecone_index.describe_index_stats()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:38:37.008394Z","iopub.execute_input":"2023-12-28T21:38:37.008939Z","iopub.status.idle":"2023-12-28T21:38:43.020085Z","shell.execute_reply.started":"2023-12-28T21:38:37.008897Z","shell.execute_reply":"2023-12-28T21:38:43.018939Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e932a88dba1f4ed59060153d1fe3f856"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"185212db6b3645e1b2c7b55245a6133c"}},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'dimension': 384,\n 'index_fullness': 0.01,\n 'namespaces': {'': {'vector_count': 1000}},\n 'total_vector_count': 1000}"},"metadata":{}}]},{"cell_type":"markdown","source":"Once the vectors are upserted, we can now query the index directly. \n\nNotice that it returns us the similarity score in the result too.","metadata":{}},{"cell_type":"code","source":"query = \"fish\"\n\n# create the query vector\nuser_query = model.encode(query).tolist()\n\n# submit the query to the Pinecone index\npinecone_answer = pinecone_index.query(user_query, top_k=3, include_metadata=True)\n\nfor result in pinecone_answer[\"matches\"]:\n    score_rounded = round(result['score'], 2)\n    print(f\"{score_rounded}, {result['metadata']['title']}\")\n    print(\"-\" * 120)","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:38:48.540384Z","iopub.execute_input":"2023-12-28T21:38:48.542293Z","iopub.status.idle":"2023-12-28T21:38:48.961869Z","shell.execute_reply.started":"2023-12-28T21:38:48.542222Z","shell.execute_reply":"2023-12-28T21:38:48.960845Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59446ba773b0424abe1ebf00b23511f9"}},"metadata":{}},{"name":"stdout","text":"0.46, Cause Of Massive Fish Kill In Shinnecock Canal Not Clear - 27 East\n------------------------------------------------------------------------------------------------------------------------\n0.39, 'Secret' life of sharks: Study reveals their surprising social networks\n------------------------------------------------------------------------------------------------------------------------\n0.3, Oh No, Earthworm Jim\n------------------------------------------------------------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Method 2: Process with Spark and write to Pinecone with Spark\n\nNow that we have seen how to `upsert` with Pinecone, you may be curious whether we can use Spark DataFrame Writer (just like Weaviate) to write the entire dataframe out in a single operation. The answer is yes -- we will now take a look at how to do that and a spoiler alert is that you will need to use a Spark connector too! \n\nWe first need to write a mapping function to map the tokenizer and embedding model onto batches of rows within the Spark DataFrame. We will be using a type of [pandas UDFs](https://www.databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html), called scalar iterator UDFs. \n\n> The function takes and outputs an iterator of pandas.Series.\n\n> The length of the whole output must be the same length of the whole input. Therefore, it can prefetch the data from the input iterator as long as the lengths of entire input and output are the same. The given function should take a single column as input.\n\n> It is also useful when the UDF execution requires expensive initialization of some state. \n\nWe load the model once per partition of data, not per [batch](https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html#setting-arrow-batch-size), which is faster. \n\nFor more documentation, refer [here](https://docs.databricks.com/udf/pandas.html).\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom pyspark.sql.functions import pandas_udf\nfrom sentence_transformers import SentenceTransformer\nfrom typing import Iterator\n\n@pandas_udf(\"array<float>\")\ndef create_embeddings_with_transformers(\n    sentences: Iterator[pd.Series],) -> Iterator[pd.Series]:\n    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n    for batch in sentences:\n        yield pd.Series(model.encode(batch).tolist())\n\nimport pyspark.sql.functions as F\n\ntransformer_type = \"sentence-transformers/all-MiniLM-L6-v2\"\nembedding_spark_df = (\n    df.limit(1000)\n    .withColumn(\"values\", create_embeddings_with_transformers(\"title\")) \n    .withColumn(\"namespace\", F.lit(None)) ## Pinecone free-tier does not support namespace\n    .withColumn(\"sparse_values\", F.lit(None)) ## required by Pinecone v2.0.1 release\n    .withColumn(\"metadata\", F.to_json(F.struct(F.col(\"topic\").alias(\"TOPIC\"))))\n    # We select these columns because they are expected by the Spark-Pinecone connector\n    .select(\"id\", \"values\", \"sparse_values\", \"namespace\", \"metadata\")\n)\ndisplay(embedding_spark_df)","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:37:34.566789Z","iopub.execute_input":"2023-12-28T21:37:34.567978Z","iopub.status.idle":"2023-12-28T21:37:34.984982Z","shell.execute_reply.started":"2023-12-28T21:37:34.567949Z","shell.execute_reply":"2023-12-28T21:37:34.984053Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"DataFrame[id: string, values: array<float>, sparse_values: void, namespace: void, metadata: string]"},"metadata":{}}]},{"cell_type":"markdown","source":"Repeat the same step as in Method 1 above to delete and recreate the index. Again, we need to delete the index because Pinecone free tier only allows one index.\n\nNote: This could take ~3 minutes. \n","metadata":{}},{"cell_type":"code","source":"pinecone_index_name = \"news\"\n\nif pinecone_index_name in pinecone.list_indexes():\n    pinecone.delete_index(pinecone_index_name)\n\n# only create index if it doesn't exist\nmodel = SentenceTransformer(transformer_type)\nif pinecone_index_name not in pinecone.list_indexes():\n    pinecone.create_index(\n        name=pinecone_index_name,\n        dimension=model.get_sentence_embedding_dimension(),\n        metric=\"cosine\",\n    )\n\n# now connect to the index\npinecone_index = pinecone.Index(pinecone_index_name)","metadata":{"execution":{"iopub.status.busy":"2023-12-28T21:37:34.986100Z","iopub.execute_input":"2023-12-28T21:37:34.986461Z","iopub.status.idle":"2023-12-28T21:37:49.727089Z","shell.execute_reply.started":"2023-12-28T21:37:34.986430Z","shell.execute_reply":"2023-12-28T21:37:49.725284Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8dcd14ced864e61a7b33abc8ae76046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90a3881231b54da69852e9418a03da16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f0b506e460743dba942f36c41f6c138"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb0f1dded8be494dadf5f62e3b14af9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19ccd1ae45014cf08522afd108224dcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a108d1e93f9487c9b7413b366e3aa53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"563886020ce44955ba143dfa05612551"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddb1eaf390a5483ba2c165c8767159db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22a530882785485bbfdd33e16e068ab9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1f2b747e6b64965bb9ff44da667c943"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a48ddff5b654399871a65a9e5b6bfad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69eb24407d8348759794ad799dcb7e6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a210fd79fd3b4c8eaa728ba07636038b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21bb51b0626a44d1ac9ae995498ff336"}},"metadata":{}}]},{"cell_type":"markdown","source":"Instead of writing in batches, you can now use Spark DataFrame Writer to write the data out to Pinecone directly.\n\n**IMPORTANT!!** You need to attach a Spark-Pinecone connector `s3://pinecone-jars/0.2.1/spark-pinecone-uberjar.jar` in the cluster you are using. Otherwise, this following command would fail. Refer to this [documentation](https://docs.pinecone.io/docs/databricks#setting-up-a-spark-cluster) and release note [here](https://github.com/pinecone-io/spark-pinecone/releases/tag/v0.2.1) if you need more information. \n","metadata":{}},{"cell_type":"code","source":"(\n    embedding_spark_df.write.option(\"pinecone.apiKey\", pinecone_api_key)\n    .option(\"pinecone.environment\", pinecone_env)\n    .option(\"pinecone.projectName\", pinecone.whoami().projectname)\n    .option(\"pinecone.indexName\", pinecone_index_name)\n    .format(\"io.pinecone.spark.pinecone.Pinecone\")\n    .mode(\"append\")\n    .save()\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}